{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# # Create empty DataFrames for each ID type\n",
    "# doi_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "# pmid_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "# pmcid_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "#\n",
    "# start_time = time.time()\n",
    "# for root, dirs, files in os.walk(directory_path):\n",
    "#     for file in tqdm(files):\n",
    "#         if file.endswith('.csv'):\n",
    "#             file_path = os.path.join(root, file)\n",
    "#             current_df = pd.read_csv(file_path)\n",
    "#\n",
    "#             # Determine the prefix and concatenate with the corresponding DataFrame\n",
    "#             df_doi = current_df[current_df['supported_id'].str.startswith('doi:')]\n",
    "#             doi_df = pd.concat([doi_df, df_doi], ignore_index=True)\n",
    "#\n",
    "#             df_pmid = current_df[current_df['supported_id'].str.startswith('pmid:')]\n",
    "#             pmid_df = pd.concat([pmid_df, df_pmid], ignore_index=True)\n",
    "#\n",
    "#             df_pmcid = current_df[current_df['supported_id'].str.startswith('pmcid:')]\n",
    "#             pmcid_df = pd.concat([pmcid_df, df_pmcid], ignore_index=True)\n",
    "#\n",
    "# print('Creating DataFrames for each ID type took: {} minutes'.format((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create DataFrames for each ID type\n",
    "We want to create three lookup tables from the CSV files mapping PIDs to OpenAlex IDs for the resources in OpenAlex. We can do so by creating three different DataFrames, one for each ID type (doi, pmid, pmcid). Each DataFrame will have two columns: the first one will contain the PID of the ID type (doi, pmid, pmcid) and the second one will contain the OpenAlex ID of the bibliographic resource it is associated with. We can later use these DataFrames to create the corresponding tables in the database.\n",
    "As the task is memory-intensive, we need to create the ID-type DataFrames one at a time. We define a function that takes in input the prefix (doi|pmid|pmcid) needed to select the rows from the input CSV files and the input directory; it returns the corresponding a single DataFrame, where each line corresponds to a bibliographic resource and stores one PID of the ID type specified as a parameter and the OpenAlex ID of the bibliographic resource it is associated with. (Only takes into consideration the IDs that can be associated with single bibliographic resources: doi, pmid, pmcid; isbn and issn are excluded!)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi','pmid','pmcid'], directory_path:str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi','pmid','pmcid']\n",
    "#         id_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     current_df = pd.read_csv(file_path)\n",
    "#\n",
    "#                     # Determine the prefix and concatenate with the corresponding DataFrame\n",
    "#                     df_id = current_df[current_df['supported_id'].str.startswith(id_type)]\n",
    "#                     id_df = pd.concat([id_df, df_id], ignore_index=True)\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Still to slow...\n",
    "Let's try another approach. In the following function we first read each csv file line by line, then we append relevant rows to a list. Then we create a DataFrame from the list of rows and return the DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi', 'pmid', 'pmcid'], directory_path: str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi', 'pmid', 'pmcid']\n",
    "#         id_data = []\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "#                         reader = csv.DictReader(csv_file, dialect='unix')\n",
    "#                         for row in reader:\n",
    "#                             if row['supported_id'].startswith(id_type):\n",
    "#                                 id_data.append(row)\n",
    "#\n",
    "#         id_df = pd.DataFrame(id_data, columns=['supported_id', 'openalex_id'])\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Still too slow (x2)...\n",
    "This still takes forever to run. The RAM saturates pretty soon, and though it is a bit faster at the beginning, before even finishing building the list it incurs in a MemoryError. Let's try to use the multiprocessing functions built in Dask to speed up the process and manage the memory issues."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import dask.dataframe as dd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi', 'pmid', 'pmcid'], directory_path: str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi', 'pmid', 'pmcid']\n",
    "#\n",
    "#         dask_df = dd.DataFrame()\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     current_df = dd.read_csv(file_path)\n",
    "#\n",
    "#                     # Determine the prefix and concatenate with the corresponding DataFrame\n",
    "#                     df_id = current_df[current_df['supported_id'].str.startswith(id_type)]\n",
    "#                     dask_df = dd.concat([dask_df, df_id], ignore_index=True)\n",
    "#\n",
    "#         id_df = dask_df.compute()  # Convert Dask DataFrame to Pandas DataFrame\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14028/3142687411.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mdask\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataframe\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mdd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdask_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'D:/oa_work_tables.*.*.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mblocksize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m25e6\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'dask'"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "dask_df = dd.read_csv('D:/oa_work_tables.*.*.csv', blocksize=25e6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 902.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 994.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 988.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 994.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 996.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 988.06it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 503.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 994.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 497.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 502.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.01it/s]\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:41<00:00,  1.03s/it]\n",
      "100%|██████████| 47/47 [01:11<00:00,  1.52s/it]\n",
      "100%|██████████| 46/46 [01:05<00:00,  1.43s/it]\n",
      "100%|██████████| 39/39 [00:47<00:00,  1.21s/it]\n",
      " 63%|██████▎   | 33/52 [05:37<03:14, 10.22s/it]  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m directory_path \u001B[38;5;241m=\u001B[39m OA_WORK_OUTPUT_FOLDER_PATH  \u001B[38;5;66;03m# Path to the directory containing the CSV files\u001B[39;00m\n\u001B[0;32m      5\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 6\u001B[0m doi_df \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_id_df\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdoi\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirectory_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCreating the DOI DataFrame took: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m minutes\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat((time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m60\u001B[39m))\n",
      "Cell \u001B[1;32mIn[4], line 26\u001B[0m, in \u001B[0;36mcreate_id_df\u001B[1;34m(id_type, directory_path)\u001B[0m\n\u001B[0;32m     24\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m reader:\n\u001B[0;32m     25\u001B[0m                     \u001B[38;5;28;01mif\u001B[39;00m row[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupported_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstartswith(id_type):\n\u001B[1;32m---> 26\u001B[0m                         \u001B[43mid_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m id_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(id_data, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupported_id\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenalex_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m id_df\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "OA_WORK_OUTPUT_FOLDER_PATH = join('D:/oa_work_tables')\n",
    "\n",
    "directory_path = OA_WORK_OUTPUT_FOLDER_PATH  # Path to the directory containing the CSV files\n",
    "\n",
    "start_time = time.time()\n",
    "doi_df = create_id_df('doi', directory_path)\n",
    "print('Creating the DOI DataFrame took: {} minutes'.format((time.time() - start_time)/60))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pmid_df = create_id_df('pmid', directory_path)\n",
    "print('Creating the PMID DataFrame took: {} minutes'.format((time.time() - start_time)/60))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pmcid_df = create_id_df('pmcid', directory_path)\n",
    "print('Creating the PMCID DataFrame took: {} minutes'.format((time.time() - start_time)/60))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print('DOI DataFrame size: {} Mb'.format(getsizeof(doi_df)/1000000))\n",
    "print('Number of DOIs (rows) in DOI DataFrame: {}'.format(doi_df.size))\n",
    "print('PMID DataFrame size: {} Mb'.format(getsizeof(pmid_df)/1000000))\n",
    "print('Number of PMIDs (rows) in PMID DataFrame: {}'.format(pmid_df.size))\n",
    "print('PMCID DataFrame size: {} Mb'.format(getsizeof(pmcid_df)/1000000))\n",
    "print('Number of PMCIDs (rows) in PCMID DataFrame: {}'.format(pmcid_df.size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the database, create the tables and insert the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sqlite3 import connect\n",
    "import time\n",
    "\n",
    "DB_PATH = 'oa_ids_tables.db'  # Path to the SQLite database file\n",
    "\n",
    "start_time = time.time()\n",
    "with connect(DB_PATH) as con:\n",
    "    doi_table_start = time.time()\n",
    "    doi_df.to_sql(\"WorksDOI\", con, if_exists=\"replace\", index=False)\n",
    "    print('Creating the DOI table took: {} seconds'.format(time.time() - doi_table_start))\n",
    "    pmid_table_start = time.time()\n",
    "    pmid_df.to_sql(\"WorksPMID\", con, if_exists=\"replace\", index=False)\n",
    "    print('Creating the PMID table took: {} seconds'.format(time.time() - pmid_table_start))\n",
    "    pmcid_table_start = time.time()\n",
    "    pmcid_df.to_sql(\"WorksPMCID\", con, if_exists=\"replace\", index=False)\n",
    "    print('Creating the PMCID table took: {} seconds'.format(time.time() - pmcid_table_start))\n",
    "\n",
    "print('Creating the tables in the database took: {} seconds'.format(time.time() - start_time))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
