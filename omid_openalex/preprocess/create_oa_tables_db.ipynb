{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Literal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrames for each ID type\n",
    "We want to create three lookup tables from the CSV files mapping PIDs to OpenAlex IDs for the resources in OpenAlex. We can do so by creating three different DataFrames, one for each ID type (doi, pmid, pmcid). Each DataFrame will have two columns: the first one will contain the PID of the ID type (doi, pmid, pmcid) and the second one will contain the OpenAlex ID of the bibliographic resource it is associated with. We can later use these DataFrames to create the corresponding tables in the database.\n",
    "As the task is memory-intensive, we need to create the ID-type DataFrames one at a time. We define a function that takes in input the prefix (doi|pmid|pmcid) needed to select the rows from the input CSV files and the input directory; it returns the corresponding a single DataFrame, where each line corresponds to a bibliographic resource and stores one PID of the ID type specified as a parameter and the OpenAlex ID of the bibliographic resource it is associated with. (Only takes into consideration the IDs that can be associated with single bibliographic resources: doi, pmid, pmcid; isbn and issn are excluded!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi','pmid','pmcid'], directory_path:str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi','pmid','pmcid']\n",
    "#         id_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     current_df = pd.read_csv(file_path)\n",
    "#\n",
    "#                     # Determine the prefix and concatenate with the corresponding DataFrame\n",
    "#                     df_id = current_df[current_df['supported_id'].str.startswith(id_type)]\n",
    "#                     id_df = pd.concat([id_df, df_id], ignore_index=True)\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to slow...\n",
    "Let's try another approach. In the following function we first read each csv file line by line, then we append relevant rows to a list. Then we create a DataFrame from the list of rows and return the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi', 'pmid', 'pmcid'], directory_path: str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi', 'pmid', 'pmcid']\n",
    "#         id_data = []\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "#                         reader = csv.DictReader(csv_file, dialect='unix')\n",
    "#                         for row in reader:\n",
    "#                             if row['supported_id'].startswith(id_type):\n",
    "#                                 id_data.append(row)\n",
    "#\n",
    "#         id_df = pd.DataFrame(id_data, columns=['supported_id', 'openalex_id'])\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still too slow (x2)...\n",
    "This still takes forever to run. The RAM saturates pretty soon, and though it is a bit faster at the beginning, before even finishing building the list it incurs in a MemoryError.\n",
    "## Change the function for better performance\n",
    "Instead of creating a single DataFrame for all the files, we can create a DataFrame for each file and append it to the database table straight away. In this way, we don't need to store all the DataFrames in memory at the same time, thus avoiding to saturate the RAM and force the system to use the swap memory.\n",
    "\n",
    "The `mapping.create_id_db_table` function creates a table (if it doesn't already exist) and names it with the name of the ID type passed as a parameter (one among \"doi\", \"pmid\" and \"pmcid\"). Then, for each csv file in the input directory, the file is converted to a pandas DataFrame and then appended to the database table. The DataFrames, each of which corresponds to a single file,are appended one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "OA_WORK_OUTPUT_FOLDER_PATH = join('D:/oa_work_tables')\n",
    "\n",
    "directory_path = OA_WORK_OUTPUT_FOLDER_PATH  # Path to the directory containing the CSV files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from mapping import create_id_db_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 75.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 143.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 129.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 123.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 154.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 46.40it/s]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n",
      "100%|██████████| 40/40 [00:52<00:00,  1.30s/it]\n",
      "100%|██████████| 47/47 [01:10<00:00,  1.51s/it]\n",
      "100%|██████████| 46/46 [01:01<00:00,  1.34s/it]\n",
      "100%|██████████| 39/39 [01:05<00:00,  1.69s/it]\n",
      "100%|██████████| 52/52 [01:18<00:00,  1.51s/it]\n",
      "100%|██████████| 40/40 [01:51<00:00,  2.80s/it]\n",
      "100%|██████████| 55/55 [00:46<00:00,  1.19it/s]\n",
      "100%|██████████| 41/41 [01:00<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for DOIs took 9.362805151939392 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for DOIs\n",
    "create_id_db_table(directory_path, 'oa_ids_tables.db', 'doi', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 330.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 330.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 328.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 284.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 198.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 171.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 157.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 186.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 190.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 167.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 145.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 170.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 79.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 101.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 98.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.99it/s]\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.17it/s]\n",
      "100%|██████████| 40/40 [00:28<00:00,  1.42it/s]\n",
      "100%|██████████| 47/47 [00:51<00:00,  1.09s/it]\n",
      "100%|██████████| 46/46 [00:50<00:00,  1.11s/it]\n",
      "100%|██████████| 39/39 [01:13<00:00,  1.88s/it]\n",
      "100%|██████████| 52/52 [00:46<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:37<00:00,  1.06it/s]\n",
      "100%|██████████| 55/55 [00:45<00:00,  1.20it/s]\n",
      "100%|██████████| 41/41 [00:43<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for PMIDs took 6.477690362930298 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for PMIDs\n",
    "create_id_db_table(directory_path, 'oa_ids_tables.db', 'pmid', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 77.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 221.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 203.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 221.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 199.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 204.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 208.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 248.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 147.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 144.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 79.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.33it/s]\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.11it/s]\n",
      "100%|██████████| 40/40 [00:24<00:00,  1.66it/s]\n",
      "100%|██████████| 47/47 [01:31<00:00,  1.94s/it]\n",
      "100%|██████████| 46/46 [00:36<00:00,  1.27it/s]\n",
      "100%|██████████| 39/39 [00:30<00:00,  1.27it/s]\n",
      "100%|██████████| 52/52 [00:48<00:00,  1.06it/s]\n",
      "100%|██████████| 40/40 [00:27<00:00,  1.43it/s]\n",
      "100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "100%|██████████| 41/41 [01:23<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for PMCIDs took 6.504606902599335 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for PMCIDs\n",
    "create_id_db_table(directory_path,'oa_ids_tables.db','pmcid', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "source_tables_dir = 'D:/oa_source_tables'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 68.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for WIKIDATAs took 0.00730438232421875 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for Wikidata Q-IDs (from Sources)\n",
    "create_id_db_table(source_tables_dir,'oa_ids_tables.db','wikidata', 'source')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 56.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 128.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 90.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for ISSNs took 0.008862519264221191 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for ISSNs (from Sources)\n",
    "create_id_db_table(source_tables_dir,'oa_ids_tables.db','issn', 'source')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorksDoi\n",
      "WorksPmid\n",
      "WorksPmcid\n",
      "SourcesWikidata\n",
      "SourcesIssn\n"
     ]
    }
   ],
   "source": [
    "# Getting the table names from the database\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('oa_ids_tables.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the query to retrieve table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all the table names\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "for name in table_names:\n",
    "    print(name[0])\n",
    "\n",
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating table's indexes\n",
    "In order to speed up the queries, we can create indexes on the columns of the tables. In particular, we can create indexes on the `supported_id` column of each table, since this is the column we will use to query the tables. Without indexes, the queries will be very slow, since the database will have to scan the entire table to find the rows that match the query. With indexes, the database will be able to find the rows that match the query much faster, since it will only have to scan the index, which is much smaller than the table itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the indexes on the tables takes a while, but not too much: creating the index for the DOI table took about 8 minute. Comment out the following code if you want to create the indexes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the index on the Wikidata table took 0.0006640275319417317 minutes\n",
      "Creating the index on the ISSN table took 0.0018349250157674154 minutes\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect('oa_ids_tables.db')\n",
    "cursor = conn.cursor()\n",
    "# start_time_idx_doi = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_doi_works ON WorksDoi(supported_id)\")\n",
    "# print(f\"Creating the index on the DOI table took {(time.time()-start_time_idx_doi)/60} minutes\")\n",
    "# start_time_idx_pmid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_pmid_works ON WorksPmid(supported_id)\")\n",
    "# print(f\"Creating the index on the PMID table took {(time.time()-start_time_idx_pmid)/60} minutes\")\n",
    "# start_time_idx_pmcid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_pmcid_works ON WorksPmcid(supported_id)\")\n",
    "# print(f\"Creating the index on the PMCID table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "# start_time_idx_pmcid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_wikidata_sources ON SourcesWikidata(supported_id)\")\n",
    "# print(f\"Creating the index on the Wikidata table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "# start_time_idx_pmcid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_issn_sources ON SourcesIssn(supported_id)\")\n",
    "# print(f\"Creating the index on the ISSN table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see if the indexes have been created and how they are named by executing the following query:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('idx_doi_works', 'WorksDoi'), ('idx_pmid_works', 'WorksPmid'), ('idx_pmcid_works', 'WorksPmcid'), ('idx_wikidata_sources', 'SourcesWikidata'), ('idx_issn_sources', 'SourcesIssn')]\n"
     ]
    }
   ],
   "source": [
    "conn = sql.connect('oa_ids_tables.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT name, tbl_name FROM sqlite_master WHERE type = 'index';\"\n",
    "cursor.execute(query)\n",
    "print(cursor.fetchall())\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stats on the Mapping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22195/22195 [06:02<00:00, 61.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Number of unique omids: 74650211\n",
      "2) Number of unique openalex_ids: 74681129\n",
      "3a) Number of multi-mapped OMIDs: 29983\n",
      "3b) Number of multi-mapped OMIDs for each value of 'type':\n",
      "journal article: 10001\n",
      "reference book: 69\n",
      "series: 87\n",
      "book series: 1058\n",
      "standard: 7\n",
      "journal: 11184\n",
      "book: 7247\n",
      "proceedings article: 106\n",
      "reference entry: 108\n",
      "book chapter: 95\n",
      "report: 12\n",
      "web content: 4\n",
      "proceedings: 4\n",
      "dataset: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'D:/map_sources_omid_openalex'\n",
    "\n",
    "# Variables to store the statistics\n",
    "omid_set = set()\n",
    "openalex_id_set = set()\n",
    "multi_mapped_omids_count = 0\n",
    "multi_mapped_oaid_by_type = {}\n",
    "\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            csv_reader = csv.reader(file, delimiter=',', dialect='unix')\n",
    "\n",
    "            # Skip the header line\n",
    "            next(csv_reader)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                omid = row[0]\n",
    "                openalex_ids = row[1].split()\n",
    "                oaid_count = len(openalex_ids)\n",
    "\n",
    "                # Update omid count\n",
    "                omid_set.add(omid)\n",
    "\n",
    "                # Update openalex_id count\n",
    "                openalex_id_set.update(openalex_ids)\n",
    "\n",
    "                # Update multi-mapped OMIDs count\n",
    "                if oaid_count > 1:\n",
    "                    multi_mapped_omids_count += 1\n",
    "\n",
    "                # Update multi-mapped OMID count by type\n",
    "                    oaid_type = row[2]\n",
    "                    multi_mapped_oaid_by_type[oaid_type] = multi_mapped_oaid_by_type.get(oaid_type, 0) + 1\n",
    "\n",
    "\n",
    "# Extracting statistics\n",
    "omid_count = len(omid_set)\n",
    "openalex_id_count = len(openalex_id_set)\n",
    "\n",
    "# Printing the statistics\n",
    "print(f\"1) Number of unique omids: {omid_count}\")\n",
    "print(f\"2) Number of unique openalex_ids: {openalex_id_count}\")\n",
    "print(f\"3a) Number of multi-mapped OMIDs: {multi_mapped_omids_count}\")\n",
    "print(\"3b) Number of multi-mapped OMIDs for each value of 'type':\")\n",
    "for oaid_type, count in multi_mapped_oaid_by_type.items():\n",
    "    print(f\"{oaid_type}: {count}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22195/22195 [07:24<00:00, 49.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the multi-mapped OMIDs took 7.412501847743988 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyse the multi-mapped OMIDs cases\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'D:/map_sources_omid_openalex'\n",
    "\n",
    "def get_multi_mapped_omids(inp_dir):\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = pd.DataFrame()\n",
    "    # Iterate over each CSV file\n",
    "    for filename in tqdm(os.listdir(inp_dir)):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(inp_dir, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                reader = csv.DictReader(file, delimiter=',', dialect='unix')\n",
    "\n",
    "                for row in reader:\n",
    "                    oaids = set(row['openalex_id'].split())\n",
    "                    if len(oaids) > 1:\n",
    "                        row_df = pd.DataFrame([row])\n",
    "                        res = pd.concat([res, row_df], ignore_index=True)\n",
    "\n",
    "    print(f\"Getting the multi-mapped OMIDs took {(time.time()-start_time)/60} minutes\")\n",
    "    return res\n",
    "\n",
    "\n",
    "multi_mapped_omids_df = get_multi_mapped_omids(directory)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                     omid                          openalex_id  \\\n0      meta:br/0620890203              W4240532345 W2009482400   \n1          meta:br/060100              W4234406162 W4211128081   \n2           meta:br/06084              W4300925974 W4300708667   \n3          meta:br/060104              W4242362061 W2407964371   \n4           meta:br/06046  W4320079736 S4210220879 W4297406376   \n...                   ...                                  ...   \n29978  meta:br/0620863862              W2093379777 W4251342724   \n29979  meta:br/0620865371                W4239257240 S17623354   \n29980  meta:br/0620861328                S54833290 W4241008919   \n29981  meta:br/0620861225              W4247384552 S4210219897   \n29982  meta:br/0620879333              W3040908417 W1868193058   \n\n                  type  \n0      journal article  \n1       reference book  \n2       reference book  \n3       reference book  \n4               series  \n...                ...  \n29978  journal article  \n29979          journal  \n29980          journal  \n29981          journal  \n29982  journal article  \n\n[29983 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>omid</th>\n      <th>openalex_id</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>meta:br/0620890203</td>\n      <td>W4240532345 W2009482400</td>\n      <td>journal article</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>meta:br/060100</td>\n      <td>W4234406162 W4211128081</td>\n      <td>reference book</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>meta:br/06084</td>\n      <td>W4300925974 W4300708667</td>\n      <td>reference book</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>meta:br/060104</td>\n      <td>W4242362061 W2407964371</td>\n      <td>reference book</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>meta:br/06046</td>\n      <td>W4320079736 S4210220879 W4297406376</td>\n      <td>series</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29978</th>\n      <td>meta:br/0620863862</td>\n      <td>W2093379777 W4251342724</td>\n      <td>journal article</td>\n    </tr>\n    <tr>\n      <th>29979</th>\n      <td>meta:br/0620865371</td>\n      <td>W4239257240 S17623354</td>\n      <td>journal</td>\n    </tr>\n    <tr>\n      <th>29980</th>\n      <td>meta:br/0620861328</td>\n      <td>S54833290 W4241008919</td>\n      <td>journal</td>\n    </tr>\n    <tr>\n      <th>29981</th>\n      <td>meta:br/0620861225</td>\n      <td>W4247384552 S4210219897</td>\n      <td>journal</td>\n    </tr>\n    <tr>\n      <th>29982</th>\n      <td>meta:br/0620879333</td>\n      <td>W3040908417 W1868193058</td>\n      <td>journal article</td>\n    </tr>\n  </tbody>\n</table>\n<p>29983 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "multi_mapped_omids_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# set display options to show all the values of larger columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# export to csv\n",
    "multi_mapped_omids_df.to_csv('multi_mapped_omids.csv', index=False, quoting=csv.QUOTE_ALL)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the total number of OAIDs in the Work folder of the OpenAlex dump\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'D:/oa_work_tables'\n",
    "\n",
    "# Variables to store the statistics\n",
    "oaids_set = set()\n",
    "for root, dirs, files in os.walk(directory):\n",
    "            for file in tqdm(files):\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    with open(csv_path, encoding='utf-8') as file:\n",
    "                        csv_reader = csv.reader(file, delimiter=',', dialect='unix')\n",
    "\n",
    "                        # Skip the header line\n",
    "                        next(csv_reader)\n",
    "\n",
    "                        for row in csv_reader:\n",
    "                            oaids_set.add(row[1])\n",
    "\n",
    "print(f\"Number of OAIDs in the Work folder of the OpenAlex dump: {len(oaids_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "omid-openalex",
   "language": "python",
   "display_name": "omid-openalex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
