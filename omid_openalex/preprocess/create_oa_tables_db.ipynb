{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import Literal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrames for each ID type\n",
    "We want to create three lookup tables from the CSV files mapping PIDs to OpenAlex IDs for the resources in OpenAlex. We can do so by creating three different DataFrames, one for each ID type (doi, pmid, pmcid). Each DataFrame will have two columns: the first one will contain the PID of the ID type (doi, pmid, pmcid) and the second one will contain the OpenAlex ID of the bibliographic resource it is associated with. We can later use these DataFrames to create the corresponding tables in the database.\n",
    "As the task is memory-intensive, we need to create the ID-type DataFrames one at a time. We define a function that takes in input the prefix (doi|pmid|pmcid) needed to select the rows from the input CSV files and the input directory; it returns the corresponding a single DataFrame, where each line corresponds to a bibliographic resource and stores one PID of the ID type specified as a parameter and the OpenAlex ID of the bibliographic resource it is associated with. (Only takes into consideration the IDs that can be associated with single bibliographic resources: doi, pmid, pmcid; isbn and issn are excluded!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi','pmid','pmcid'], directory_path:str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi','pmid','pmcid']\n",
    "#         id_df = pd.DataFrame(columns=['supported_id', 'openalex_id'])\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     current_df = pd.read_csv(file_path)\n",
    "#\n",
    "#                     # Determine the prefix and concatenate with the corresponding DataFrame\n",
    "#                     df_id = current_df[current_df['supported_id'].str.startswith(id_type)]\n",
    "#                     id_df = pd.concat([id_df, df_id], ignore_index=True)\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to slow...\n",
    "Let's try another approach. In the following function we first read each csv file line by line, then we append relevant rows to a list. Then we create a DataFrame from the list of rows and return the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import csv\n",
    "# from tqdm import tqdm\n",
    "# from typing import Literal\n",
    "#\n",
    "# def create_id_df(id_type: Literal['doi', 'pmid', 'pmcid'], directory_path: str):\n",
    "#     \"\"\"\n",
    "#     Create a DataFrame for a given ID type (doi, pmid, pmcid) from the CSV files in the input directory.\n",
    "#     :param id_type: the ID type (doi, pmid, pmcid)\n",
    "#     :param directory_path: the path to the directory containing the CSV files\n",
    "#     :return: a DataFrame containing the PIDs of the ID type specified as a parameter and the OpenAlex IDs of the\n",
    "#     bibliographic resources they are associated with\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         assert id_type in ['doi', 'pmid', 'pmcid']\n",
    "#         id_data = []\n",
    "#         for root, dirs, files in os.walk(directory_path):\n",
    "#             for file in tqdm(files):\n",
    "#                 if file.endswith('.csv'):\n",
    "#                     file_path = os.path.join(root, file)\n",
    "#                     with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "#                         reader = csv.DictReader(csv_file, dialect='unix')\n",
    "#                         for row in reader:\n",
    "#                             if row['supported_id'].startswith(id_type):\n",
    "#                                 id_data.append(row)\n",
    "#\n",
    "#         id_df = pd.DataFrame(id_data, columns=['supported_id', 'openalex_id'])\n",
    "#         return id_df\n",
    "#     except AssertionError:\n",
    "#         print('The ID type must be one of the following: doi, pmid, pmcid')\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still too slow (x2)...\n",
    "This still takes forever to run. The RAM saturates pretty soon, and though it is a bit faster at the beginning, before even finishing building the list it incurs in a MemoryError.\n",
    "## Change the function for better performance\n",
    "Instead of creating a single DataFrame for all the files, we can create a DataFrame for each file and append it to the database table straight away. In this way, we don't need to store all the DataFrames in memory at the same time, thus avoiding to saturate the RAM and force the system to use the swap memory.\n",
    "\n",
    "The following function creates a table (if it doesn't already exist) and names it with the name of the ID type passed as a parameter (one among \"doi\", \"pmid\" and \"pmcid\"). Then, for each csv file in the input directory, the file is converted to a pandas DataFrame and then appended to the database table. The DataFrames, each of which corresponds to a single file,are appended one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "OA_WORK_OUTPUT_FOLDER_PATH = join('D:/oa_work_tables')\n",
    "\n",
    "directory_path = OA_WORK_OUTPUT_FOLDER_PATH  # Path to the directory containing the CSV files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_id_db_table(inp_dir:str, db_path:str, id_type:Literal['doi', 'pmid', 'pmcid', 'wikidata', 'issn'], entity_type: Literal['work', 'source'])-> None:\n",
    "\n",
    "    table_name = f'{entity_type.capitalize()}s{id_type.capitalize()}'\n",
    "    start_time = time.time()\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (table_name,))\n",
    "        if cursor.fetchone():\n",
    "            raise ValueError(f\"Table {table_name} already exists\")\n",
    "\n",
    "        for root, dirs, files in os.walk(inp_dir):\n",
    "            for file in tqdm(files):\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    file_df = pd.read_csv(csv_path)  # Read the CSV file into a DataFrame\n",
    "\n",
    "                    # Select only the rows with the ID type specified as a parameter and create a new DataFrame\n",
    "                    id_df = file_df[file_df['supported_id'].str.startswith(id_type)]\n",
    "\n",
    "                    # Append the DataFrame's rows to the existing table in the database\n",
    "                    id_df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "\n",
    "    print(f\"Creating the database table for {id_type.upper()}s took {(time.time()-start_time)/60} minutes\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 16.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 75.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 143.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 129.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 123.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 133.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 110.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 154.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 132.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 62.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 46.40it/s]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.06s/it]\n",
      "100%|██████████| 40/40 [00:52<00:00,  1.30s/it]\n",
      "100%|██████████| 47/47 [01:10<00:00,  1.51s/it]\n",
      "100%|██████████| 46/46 [01:01<00:00,  1.34s/it]\n",
      "100%|██████████| 39/39 [01:05<00:00,  1.69s/it]\n",
      "100%|██████████| 52/52 [01:18<00:00,  1.51s/it]\n",
      "100%|██████████| 40/40 [01:51<00:00,  2.80s/it]\n",
      "100%|██████████| 55/55 [00:46<00:00,  1.19it/s]\n",
      "100%|██████████| 41/41 [01:00<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for DOIs took 9.362805151939392 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for DOIs\n",
    "create_id_db_table(directory_path, 'oa_ids_tables.db', 'doi', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 330.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 330.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 328.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 284.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 198.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 171.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 157.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 186.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 190.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 167.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 145.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 170.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 79.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 101.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 98.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.99it/s]\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.17it/s]\n",
      "100%|██████████| 40/40 [00:28<00:00,  1.42it/s]\n",
      "100%|██████████| 47/47 [00:51<00:00,  1.09s/it]\n",
      "100%|██████████| 46/46 [00:50<00:00,  1.11s/it]\n",
      "100%|██████████| 39/39 [01:13<00:00,  1.88s/it]\n",
      "100%|██████████| 52/52 [00:46<00:00,  1.13it/s]\n",
      "100%|██████████| 40/40 [00:37<00:00,  1.06it/s]\n",
      "100%|██████████| 55/55 [00:45<00:00,  1.20it/s]\n",
      "100%|██████████| 41/41 [00:43<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for PMIDs took 6.477690362930298 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for PMIDs\n",
    "create_id_db_table(directory_path, 'oa_ids_tables.db', 'pmid', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 77.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 221.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 203.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 329.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 221.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 199.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.91it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 204.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 208.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 248.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 249.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 153.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 147.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 125.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 144.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 79.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 76.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 107.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 61.33it/s]\n",
      "100%|██████████| 13/13 [00:11<00:00,  1.11it/s]\n",
      "100%|██████████| 40/40 [00:24<00:00,  1.66it/s]\n",
      "100%|██████████| 47/47 [01:31<00:00,  1.94s/it]\n",
      "100%|██████████| 46/46 [00:36<00:00,  1.27it/s]\n",
      "100%|██████████| 39/39 [00:30<00:00,  1.27it/s]\n",
      "100%|██████████| 52/52 [00:48<00:00,  1.06it/s]\n",
      "100%|██████████| 40/40 [00:27<00:00,  1.43it/s]\n",
      "100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "100%|██████████| 41/41 [01:23<00:00,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for PMCIDs took 6.504606902599335 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for PMCIDs\n",
    "create_id_db_table(directory_path,'oa_ids_tables.db','pmcid', 'work')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "source_tables_dir = 'D:/oa_source_tables'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 68.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 53.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for WIKIDATAs took 0.00730438232421875 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for Wikidata Q-IDs (from Sources)\n",
    "create_id_db_table(source_tables_dir,'oa_ids_tables.db','wikidata', 'source')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 41.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 56.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 128.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 90.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the database table for ISSNs took 0.008862519264221191 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create database table for ISSNs (from Sources)\n",
    "create_id_db_table(source_tables_dir,'oa_ids_tables.db','issn', 'source')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorksDoi\n",
      "WorksPmid\n",
      "WorksPmcid\n",
      "SourcesWikidata\n",
      "SourcesIssn\n"
     ]
    }
   ],
   "source": [
    "# Getting the table names from the database\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('oa_ids_tables.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute the query to retrieve table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Fetch all the table names\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "for name in table_names:\n",
    "    print(name[0])\n",
    "\n",
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating table's indexes\n",
    "In order to speed up the queries, we can create indexes on the columns of the tables. In particular, we can create indexes on the `supported_id` column of each table, since this is the column we will use to query the tables. Without indexes, the queries will be very slow, since the database will have to scan the entire table to find the rows that match the query. With indexes, the database will be able to find the rows that match the query much faster, since it will only have to scan the index, which is much smaller than the table itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the indexes on the tables takes a while, but not too much: creating the index for the DOI table took about 8 minute. Comment out the following code if you want to create the indexes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the index on the Wikidata table took 0.0006640275319417317 minutes\n",
      "Creating the index on the ISSN table took 0.0018349250157674154 minutes\n"
     ]
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "conn = sql.connect('oa_ids_tables.db')\n",
    "cursor = conn.cursor()\n",
    "# start_time_idx_doi = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_doi_works ON WorksDoi(supported_id)\")\n",
    "# print(f\"Creating the index on the DOI table took {(time.time()-start_time_idx_doi)/60} minutes\")\n",
    "# start_time_idx_pmid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_pmid_works ON WorksPmid(supported_id)\")\n",
    "# print(f\"Creating the index on the PMID table took {(time.time()-start_time_idx_pmid)/60} minutes\")\n",
    "# start_time_idx_pmcid = time.time()\n",
    "# cursor.execute(\"CREATE INDEX idx_pmcid_works ON WorksPmcid(supported_id)\")\n",
    "# print(f\"Creating the index on the PMCID table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "start_time_idx_pmcid = time.time()\n",
    "cursor.execute(\"CREATE INDEX idx_wikidata_sources ON SourcesWikidata(supported_id)\")\n",
    "print(f\"Creating the index on the Wikidata table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "start_time_idx_pmcid = time.time()\n",
    "cursor.execute(\"CREATE INDEX idx_issn_sources ON SourcesIssn(supported_id)\")\n",
    "print(f\"Creating the index on the ISSN table took {(time.time()-start_time_idx_pmcid)/60} minutes\")\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see if the indexes have been created and how they are named by executing the following query:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('idx_doi_works', 'WorksDoi'), ('idx_pmid_works', 'WorksPmid'), ('idx_pmcid_works', 'WorksPmcid'), ('idx_wikidata_sources', 'SourcesWikidata'), ('idx_issn_sources', 'SourcesIssn')]\n"
     ]
    }
   ],
   "source": [
    "conn = sql.connect('oa_ids_tables.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT name, tbl_name FROM sqlite_master WHERE type = 'index';\"\n",
    "cursor.execute(query)\n",
    "print(cursor.fetchall())\n",
    "cursor.close()\n",
    "conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stats on the Mapping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22195/22195 [05:52<00:00, 62.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Number of unique omids: 74567082\n",
      "2) Number of unique openalex_ids: 74586242\n",
      "3a) Number of multi-mapped oaids: 17897\n",
      "3b) Number of multi-mapped oaids for each value of 'type':\n",
      "reference book: 69\n",
      "series: 1\n",
      "standard: 7\n",
      "book series: 1\n",
      "journal: 241\n",
      "journal article: 10001\n",
      "book: 7247\n",
      "proceedings article: 106\n",
      "reference entry: 108\n",
      "book chapter: 95\n",
      "report: 12\n",
      "web content: 4\n",
      "proceedings: 4\n",
      "dataset: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'D:/omid_openalex_mapping'\n",
    "\n",
    "# Variables to store the statistics\n",
    "omid_set = set()\n",
    "openalex_id_set = set()\n",
    "multi_mapped_omids_count = 0\n",
    "multi_mapped_oaid_by_type = {}\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            csv_reader = csv.reader(file, delimiter=',', dialect='unix')\n",
    "\n",
    "            # Skip the header line\n",
    "            next(csv_reader)\n",
    "\n",
    "            for row in csv_reader:\n",
    "                omid = row[0]\n",
    "                openalex_ids = row[1].split()\n",
    "                oaid_count = len(openalex_ids)\n",
    "\n",
    "                # Update omid count\n",
    "                omid_set.add(omid)\n",
    "\n",
    "                # Update openalex_id count\n",
    "                openalex_id_set.update(openalex_ids)\n",
    "\n",
    "                # Update multi-mapped OMIDs count\n",
    "                if oaid_count > 1:\n",
    "                    multi_mapped_omids_count += 1\n",
    "\n",
    "                # Update multi-mapped OMID count by type\n",
    "                    oaid_type = row[2]\n",
    "                    multi_mapped_oaid_by_type[oaid_type] = multi_mapped_oaid_by_type.get(oaid_type, 0) + 1\n",
    "\n",
    "# Extracting statistics\n",
    "omid_count = len(omid_set)\n",
    "openalex_id_count = len(openalex_id_set)\n",
    "\n",
    "# Printing the statistics\n",
    "print(f\"1) Number of unique omids: {omid_count}\")\n",
    "print(f\"2) Number of unique openalex_ids: {openalex_id_count}\")\n",
    "print(f\"3a) Number of multi-mapped OMIDs: {multi_mapped_omids_count}\")\n",
    "print(\"3b) Number of multi-mapped OMIDs for each value of 'type':\")\n",
    "for oaid_type, count in multi_mapped_oaid_by_type.items():\n",
    "    print(f\"{oaid_type}: {count}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting the total number of OAIDs in the Work folder of the OpenAlex dump\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "directory = 'D:/oa_work_tables'\n",
    "\n",
    "# Variables to store the statistics\n",
    "oaids_set = set()\n",
    "for root, dirs, files in os.walk(directory):\n",
    "            for file in tqdm(files):\n",
    "                if file.endswith('.csv'):\n",
    "                    csv_path = os.path.join(root, file)\n",
    "                    with open(csv_path, encoding='utf-8') as file:\n",
    "                        csv_reader = csv.reader(file, delimiter=',', dialect='unix')\n",
    "\n",
    "                        # Skip the header line\n",
    "                        next(csv_reader)\n",
    "\n",
    "                        for row in csv_reader:\n",
    "                            oaids_set.add(row[1])\n",
    "\n",
    "print(f\"Number of OAIDs in the Work folder of the OpenAlex dump: {len(oaids_set)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "omid-openalex",
   "language": "python",
   "display_name": "omid-openalex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
